#统计学习方法
[TOC]

##感知机

先任意选取一个超平面，然后用梯度下降法不断极小化目标函数.在这个过程中一次随机选取一个误分类点使其梯度下降，经过有限次搜索可以找到将训练数据完全正确分开的分离超平面

$$T=\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\}$$

其中$x_{i}\in{\cal X}=\mathbf{R}^{i}\:,\quad y_{i}\in{\cal Y}=\{-1,1\}\:,\quad i=1,2,\cdots,N\:,$求参数$w\:,b\:$，以及是以下损失函数得最小值，其几何含义即为所有误分类点与分解面得距离之和最小

$$min_{w,b}\:L(w,b)=-\sum_{x_i\in M}y_i(w\bullet x_i+b)$$
如果$y_{i}(w\cdot x_{i}+b)\leqslant0$
$$\begin{array}{l}w\leftarrow w+\eta y_i x_i\\ b\leftarrow b+\eta y_i\end{array}$$
其中M为误分类点得集合。学习率（步长）$\eta\:(0\:\:<\:\eta\:\leq\:1)$ 。本质上是不停调整分界线，使整个平面上不存在误分类点。当训练数据集线性可分时，感知机学习算法原始形式法代是收敛的，训练集线性不可分时，感知机学习算法不收敛， 法代结果 会发生震荡

计算方便可采用对偶形式：令$w\:,b\:=0$，经过n次修改，$\alpha_{i}=n_{i}\eta.$
$$\begin{array}{c}w=\sum_{i=1}^N\alpha_i y_i x_i\\ b=\sum_{i=1}^N\alpha_i y_i\end{array}$$

带入原始形式其余一致，初值为0，迭代次数应该选大一点

## $k$近邻法
定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的$k$个实例，这$k$个实例的多数属于某个类，就把该输入实例分为这个类。$k$近邻法没有显式的学习过程

k 近邻模型对应于基于训练数据集对特征空间的一个划分 ki丘邻法中， 当训练集、距离度量、 k 值 及分类决策规则确定后，其结果唯一确定

kd 树是一种便于 对 k 维空间中的数据进行快速检索的数据结. kd 树是二叉树，表示对 k 维空间 的一个划分，其每个结点对应于 k 维空间划分中的 一个超矩形区域

## 朴素贝叶斯法
首先基于特征条件独立假设学习输入/输出的联合概率分布:然后基于此模型，对给定的输入 x，利用贝叶斯定理求出后验概率最大的输出y。后验概率最大即等价于0-1随时函数的期望风险最小

贝叶斯定理：
$$P(A|B)=\frac{P(B|A)*P(A)}{P(B)}$$
其中A为类别，B为特征。

朴素：条件独立性假设指各个特征之间相互独立，但该假设较强让方法简明的同时，可能丧失部分准确性
$$ P(X^{(1)}=x^{(1)},\cdots,X^{(n)}=x^{(n)}\mid Y=c_{k})=\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}\mid Y=c_{_k})$$

朴素贝叶斯分类方法的基本公式：
$$P(Y=c_k\mid X=x)=\frac{P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}\mid Y=c_k)}{\sum_k P(Y=c_k)\prod_j P(X^{(j)}=x^{(j)}\mid Y=c_k)},\quad k=1,2,\cdots,K$$

分类只关心类别$Y=c_{k}$,分母与$Y$的取值无关，为一个常数，所以定义朴素贝叶斯分类器：
$$y=\arg\max_{c_{i}}P(Y=c_{k})\prod_{j}P(X^{(j)}=x^{(j)}\mid Y=c_{k})$$

算法：
1. 计算先验概率$P(Y=c_{k})$和条件概率$P(X^{(j)}=a_{jl}\mid Y=c_{k})$
2. 对给定的实例$x$计算$P(Y=c_{k})\prod_{j=1}^{n}P(X^{(j)}=x^{(j)}\mid Y=c_{k})$
3. 取最大后验概率确定$x$的类型$c_{k}$

## 决策树
### 概念
决策树是从训练数据中归纳出一套**完整且互斥**的分类规则，本质上是由数据集估计出条件概率模型；决策树学习的损失函数一般用正则化的极大似然函数，追求损失函数最小；最有决策树是NP complete问题，现实只能获得次优解
算法常用ID3、C4.5、CART

**特征选择**：可以在学习开始对特征进行选择，只留下足够训练的特征，节省计算量

**决策树生成**：构建过程为递归地选择最优特征并根据该特征分割训练数据，从根节点地数据选一个最优特征把根节点地数据分成子集，继续分别对各个子集进行各自最优特征地分割，直到把所有数据被分道叶节点上

**剪枝**：为防止过拟合，需要对生成地决策树进行修剪，使其具有更好的泛化能力。即去掉过于精细的子节点，把数据退回父节点上

### 特征选择

特征选择的准则是选择信息增益最大的特征，其往往有更强的分类能力；也可用信息增益比进行进一步校正，信息(information gain)表示知道特征A的信息而使得D经验熵（不确定性）减小的程度
$$增益：g(D,A)=H(D)-H(D\mid A)$$
$$熵：H(p)=-\sum_{i=1}^n p_i\log p_i$$
$$条件熵：H(Y\mid X)=\sum_{i=1}^n p_i H(Y\mid X=x_i)$$
$$增益比: g_R(D,A)\:=\:\dfrac{g(D,A)}{H(D)}$$
p为随机变量X的概率分布，熵H(P)越大，变量的不确定性就大$0\leqslant H(p)\leqslant\log n$

随机变量 X 给定的条件下随机变量 Y 的是件熵 (conditionaJ entropy)$H(Y|X) $ ， 定义为 X 给定条件下 Y 的条件概率分布的娟对 X 的数学期望。
